{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Analysis\n",
    "\n",
    "In this example, we optimize a workflow for [FinRobot](https://github.com/AI4Finance-Foundation/FinRobot), an agentic application that performs several complex tasks, including market forecasting, document analysis, and portfolio management. \n",
    "\n",
    "This workflow is very dynamic. There is a group leader that can call upon 11 possible agents in any order it chooses. This means there can be a different LLM pipeline for each end-user request. The leader will loop over different agents from the agent pool until it determines that the task is complete or until a fixed timeout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![finrobot](../imgs/finrobot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, set the environment variable for your `OPENAI_API_KEY`. Then, execute `./prepare_data.sh`. This loads the [FinGPT Dataset](https://huggingface.co/FinGPT) from HuggingFace. Then, import Cognify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cognify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loader\n",
    "\n",
    "The original dataset contains a variety of tasks. Specifically, we look at sentiment analysis, headline classification, and financial QA (FiQA). To conduct a holistic assessment of the workflow, we combine these datasets into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_specific_data(task, mode):\n",
    "    sentiment_df = pd.read_parquet(f\"data/{task}.parquet\")\n",
    "    data = []\n",
    "    for i, row in sentiment_df.iterrows():\n",
    "        input = {\n",
    "            'task': row['instruction'] + \"\\n\" + row['input'],\n",
    "            'mode': mode\n",
    "        }\n",
    "        output = {\n",
    "            'label': row['output']\n",
    "        }\n",
    "        data.append((input, output))\n",
    "        if i == 99:\n",
    "            break\n",
    "    return data\n",
    "\n",
    "@cognify.register_data_loader\n",
    "def load_all_data():\n",
    "    sentiment_data = load_specific_data('sentiment', 'sentiment_analysis')\n",
    "    headline_data = load_specific_data('headline', 'headline_classification')\n",
    "    fiqa_data = load_specific_data('fiqa', 'fiqa')\n",
    "\n",
    "    trainset = sentiment_data[:70] + headline_data[:70] + fiqa_data[:70]\n",
    "    devset = sentiment_data[70:85] + headline_data[70:85] + fiqa_data[70:85]\n",
    "    testset = sentiment_data[85:] + headline_data[85:] + fiqa_data[85:]\n",
    "    return trainset, devset, testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentiment analysis and headline classification, we use F1-score as the evaluator. For FiQA, we use LLM-as-judge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import litellm\n",
    "from cognify.hub.evaluators import f1_score_str\n",
    "\n",
    "def evaluate_sentiment(answer, label):\n",
    "    return f1_score_str(answer, label)\n",
    "\n",
    "def evaluate_headline(answer, label):\n",
    "    return f1_score_str(answer, label)\n",
    "\n",
    "from pydantic import BaseModel\n",
    "class Assessment(BaseModel):\n",
    "    success: bool\n",
    "\n",
    "def evaluate_fiqa(answer, label, task):\n",
    "    system_prompt=\"Given the question and the ground truth, evaluate if the response answers the question.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"You're given the following inputs:\\n\\nQuestion: \" + task + \"\\n\\nGround Truth: \" + label + \"\\n\\nResponse: \" + answer}]\n",
    "    response = litellm.completion('gpt-4o-mini', messages=messages, temperature=0, response_format=Assessment)\n",
    "    assessment = json.loads(response.choices[0].message.content)\n",
    "    return int(assessment['success'])\n",
    "\n",
    "@cognify.register_evaluator\n",
    "def evaluate_all_tasks(answer, label, mode, task):\n",
    "    if mode == 'sentiment_analysis':\n",
    "        return evaluate_sentiment(answer, label)\n",
    "    elif mode == 'headline_classification':\n",
    "        return evaluate_headline(answer, label)\n",
    "    elif mode == 'fiqa':\n",
    "        return evaluate_fiqa(answer, label, task)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuring the Optimizer\n",
    "\n",
    "For this task, we stick with the default search settings. This searches over whether to include Chain-of-Thought reasoning and/or few-shot examples for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognify.hub.search import default\n",
    "\n",
    "search_settings = default.create_search(\n",
    "    search_type='light',\n",
    "    n_trials=10,\n",
    "    opt_log_dir=\"optimization_results\",\n",
    "    evaluator_batch_size=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimize the Workflow\n",
    "\n",
    "The code blocks above are provided in `config.py`, along with the workflow itself in `workflow.py`. We recommend using the Cognify command-line interface (CLI) to start the optimization, like so:\n",
    "\n",
    "```console\n",
    "$ cognify optimize workflow.py\n",
    "```\n",
    "\n",
    "Alternatively, you can run the following cell (**warning**: this workflow may run for quite some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, dev = load_all_data()\n",
    "\n",
    "opt_cost, pareto_frontier, opt_logs = cognify.optimize(\n",
    "    script_path=\"workflow.py\",\n",
    "    control_param=search_settings,\n",
    "    train_set=train,\n",
    "    val_set=val,\n",
    "    eval_fn=evaluate_all_tasks,\n",
    "    force=True, # This will overwrite logs for any existing results\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cog-source",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
